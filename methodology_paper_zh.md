--
## 1. Agent Overview

### 1.1 SUBER框架架构

我们的SUBER（Simulation-based User Behavior and Emotion-aware Recommendation）框架是一个基于大语言模型的交互式推荐环境，集成了情感感知记忆和检索机制。该框架的核心组件包括：

- **环境模拟器（Simulatio4RecSys）**：基于Gymnasium的强化学习环境，提供标准化的观察空间、动作空间和奖励函数，支持多用户并行交互和动态环境状态管理
- **用户画像生成器**：动态生成具有个性化特征的用户，包括年龄、性别、职业、爱好等基础属性，以及活动水平、从众程度、多样性偏好等情感特征，确保用户群体的多样性和代表性
- **物品检索模块**：基于内容相似性、时间衰减和情感一致性的多维度检索，支持语义向量检索、协同过滤和情感标签匹配，实现精准的物品推荐候选集生成
- **情感记忆组件**：维护用户交互历史的情感标签，自动推断用户的情感状态（效价和唤醒度），建立情感一致性评分机制，实现基于情感偏好的个性化推荐
- **LLM评分器**：基于检索历史的推理评分，采用Chain of Thought推理策略，结合用户画像和观影历史，生成可解释的评分结果和推荐理由
- **强化学习代理**：多种算法（DQN、PPO、A2C等）的推荐策略学习，通过环境交互学习最优推荐策略，支持在线学习和策略更新，实现推荐系统的持续优化

![SUBER框架架构](framework.png)

### 1.2 核心工作流程

在每个交互步骤中，系统执行以下流程：
1. **历史交互检索**：根据当前候选物品的内容特征（类型、导演、演员、描述等），从用户的历史交互记录中检索K个最相关的电影评分，考虑内容相似性、时间衰减和情感一致性三个维度
2. **Prompt格式化**：将检索到的历史交互按照时间顺序格式化，包含电影标题、年份、评分、类型、导演、描述等详细信息，构建结构化的约束性prompt输入
3. **LLM推理评分**：将用户画像、检索历史和当前候选物品信息输入LLM，采用Chain of Thought推理策略，生成1-3个评分理由和对应的数值评分
4. **情感记忆更新**：根据LLM生成的评分，自动推断用户的情感状态（效价和唤醒度），更新用户的情感记忆库，维护情感标签的时间序列
5. **闭环推荐优化**：基于当前交互结果和情感记忆状态，调整推荐策略，为下一个推荐周期提供更精准的物品候选集，实现推荐系统的自适应优化

## 2. 用户画像

### 2.1 用户构造方式

我们采用基于LLM的用户生成方法，通过三个核心字典来构建用户画像：

为了让LLM稳定地“演好同一个人”，我们把提示词拆成三部分并各司其职：基础属性像身份证，固定这个人的基本画像（如“陈晨，27岁，M，理工科背景，性格踏实，表达简练”）；职业爱好提供生活背景与兴趣触发点（如“后端工程师，爱攀岩与理性科普，偶尔看科幻”）；情感特征则规定评分时的行为风格（如“活动水平2，从众2，多样性2：不轻易给高分，但遇到契合兴趣的书会耐心读完，偶尔尝试相近领域”）。当系统为陈晨推荐《自控力》时，Prompt 会同时携带这三类信息：LLM更可能给出“结构清晰、可操作性强”的正向理由并打出7/9；若推荐《玄幻爽文合集》，则基于兴趣与风格，理由会转向“与偏好不符、叙事套路感强”，评分走低。这样的分拆让角色一致、可控、可复现，也便于我们在实验中系统地“换人”：只需替换字典条目，就能批量生成不同且自洽的用户。

#### 2.1.1 基础属性字典

基础属性字典包含用户的基本信息，包括用户姓名、性别（M/F）、年龄（4-75岁）和个性化描述文本。不同性别对于同一件物品的喜好程度不同，男性更加理性使得具有逻辑性或者实用性的物品对他们更有吸引力。女性更加感性，倾向于情感的交流更喜欢直观的感受，更倾向于能够让他们带来心情愉悦的物品。对于不同年龄段的用户也会对物品的评分产生不同的影响，青少年正处在个性塑造与社交需求旺盛的时期。他们更关注潮流、个性化表达和同龄群体的认同感。青年群体开始拥有独立的消费能力和多元的兴趣，他们的喜好呈现两极化：一方面追求实用、高性价比的物品，另一方面注重自我表达和生活品质。中年用户更加注重物品的实用性、耐用性。老年用户对复杂或过度创新的产品接受度较低。

#### 2.1.2 职业爱好字典

职业爱好字典包含用户的职业类型（从预定义职业列表采样）、兴趣爱好（从预定义爱好列表采样）和儿童友好爱好（针对年轻用户）。这些特征不仅用于理解生活方式与偏好，还直接影响候选物品的匹配逻辑与评分理由生成：职业决定用户熟悉的语境、专业门槛、时间节律与疲劳阈值（如医生/客服更偏好碎片化短篇与实用综述，程序员/设计师更偏好系统化方法论、工具书与案例，教师/家长更关注教育、儿童心理与家庭主题）；爱好提供“触发器/拒斥项”信号（攀岩/户外偏好探索、自我提升与风险管理主题；音乐/艺术偏好情感表达、人物传记与回忆；篮球/电竞偏好团队协作、竞技叙事与节奏更快的内容）；children_friendly 则约束可推荐池与措辞强度，优先家庭友好、低暴力、正向价值观的条目。我们在 Prompt 中显式注入 profession + hobbies + children_friendly 三元组，并将其作为检索与LLM打分的先验：当候选与职业任务域或爱好语义相近时，提高匹配度与理由置信度；不相近则触发降权与负向理由（如“与工作流不相关”“节奏/刺激度过高，不适龄”）。同时结合使用情境（通勤/周末）与时段，调整对篇幅、难度与节奏的容忍度，使同一候选在不同用户上呈现稳定、可解释的一致性倾向。-

#### 2.1.3 情感特征字典

情感特征字典包含三个关键维度，每个维度都有详细的用户类型描述：

**活动水平（Activity Level）**：反映用户对电影推荐的接受程度和系统使用频率
- **1级（低活动水平，30%）**：极其罕见的偶尔观看者，几乎不会被电影推荐吸引，观看电影几乎成为传奇事件。电影观看习惯极其罕见，即使只是稍微不满意也会立即退出推荐系统。这类用户主要分布在年龄较大（45岁以上）和职业繁忙（如医生、律师、高管）的群体中。
- **2级（中等活动水平，50%）**：偶尔观看者，很少被电影推荐吸引。只对严格符合口味的电影感到好奇。电影观看习惯不是特别罕见，当有几次不满意经历时倾向于退出推荐系统。这类用户是主流群体，包括大部分成年用户和部分青少年用户，他们有一定的观影需求但要求较高。
- **3级（高活动水平，20%）**：电影爱好者，对电影有着永不满足的渴望，愿意观看几乎所有推荐给他们的电影。电影是生活的核心部分，电影推荐对存在至关重要。对推荐系统有容忍度，即使有一些不满意记忆也不容易退出推荐系统。这类用户主要分布在18-35岁的年轻群体中，特别是学生、艺术家、媒体从业者等职业。

**从众程度（Conformity Level）**：反映用户评分时对历史评分的依赖程度
- **1级（低从众程度，25%）**：忠实追随者，评分严重依赖电影历史评分，很少表达独立意见。通常给出与历史评分相同的评分。这类用户主要分布在年轻群体（13-17岁）和职业稳定的用户中，他们倾向于跟随大众意见，缺乏独立的艺术判断。
- **2级（中等从众程度，50%）**：平衡评估者，在给电影评分时同时考虑历史评分和个人偏好。有时给出与历史评分不同的评分。这类用户是主流群体，包括大部分成年用户，他们能够平衡社会影响和个人判断，评分相对客观。
- **3级（高从众程度，25%）**：特立独行的评论家，完全忽略历史评分，仅基于自己的品味评估电影。通常给出与历史评分差异很大的评分。这类用户主要分布在艺术相关职业（如导演、影评人、艺术学生）和具有强烈个性的用户群体中。

**多样性偏好（Diversity Level）**：反映用户对电影类型多样性的接受程度
- **1级（低多样性偏好，25%）**：极其挑剔的选择性观看者，观看电影的选择性达到排他性水平。电影选择经过精心策划以匹配个人品味，不允许任何多样性暗示。这类用户主要分布在年龄较大（35岁以上）和职业保守的用户群体中，他们偏好稳定和可预测的观影体验。
- **2级（中等多样性偏好，50%）**：小众探索者，偶尔探索不同类型，主要坚持偏好的电影类型。这类用户是主流群体，包括大部分成年用户，他们愿意在安全范围内尝试新类型，但不会偏离太远。
- **3级（高多样性偏好，25%）**：电影开拓者，在电影世界中不懈寻求独特和晦涩作品的探索者。电影选择如此多样和前卫，以至于无法分类。这类用户主要分布在年轻群体（18-30岁）和艺术、媒体、教育等创意职业中，他们追求新颖和独特的观影体验。

### 2.2 用户属性初始化比例

我们基于实际数据集统计和心理学研究，设置了合理的用户属性分布比例：

**年龄分布**：基于人口统计学数据，7个年龄段加上随机偏移
- **4-12岁（儿童）**：15%，主要关注家庭友好和动画类型电影
- **13-17岁（青少年）**：20%，偏好动作、冒险和青春类型
- **18-24岁（青年）**：25%，喜欢多样化的电影类型，包括独立电影
- **25-34岁（成年早期）**：20%，关注剧情深度和艺术价值
- **35-44岁（成年中期）**：12%，偏好经典电影和成熟主题
- **45-54岁（成年后期）**：5%，倾向于怀旧和经典类型
- **55岁以上（老年）**：3%，主要观看传统和经典电影

**性别分布**：基于实际用户数据统计
- **男性（M）**：52%，在动作、科幻、惊悚类型上偏好更强
- **女性（F）**：48%，在浪漫、喜剧、剧情类型上偏好更强

**职业爱好分布**：从预定义数据库采样，确保多样性
- **职业类型**：包含50+种职业，如工程师、教师、医生、艺术家、学生等
- **兴趣爱好**：包含30+种爱好，如运动、阅读、音乐、旅行、烹饪等
- **儿童友好爱好**：针对年轻用户，包含15+种适合儿童的爱好

**情感特征分布**：基于心理学研究和用户行为分析
- **活动水平**：1级（30%）、2级（50%）、3级（20%）
- **从众程度**：1级（25%）、2级（50%）、3级（25%）
- **多样性偏好**：1级（25%）、2级（50%）、3级（25%）

这些分布比例确保了用户群体的多样性和代表性，能够覆盖不同年龄段、性别、职业和情感特征的用户类型，为推荐系统的泛化能力提供基础。

### 2.2.1 用户特征生成算法

我们实现了一个基于分层抽样的用户特征生成算法，确保生成的用户群体严格遵循预设的分布比例。该算法采用数学化的分层抽样策略，通过精确的分布控制生成具有统计学代表性的用户群体。

**算法数学描述**：

设总用户数为 N = 600，各特征维度的分布比例定义如下：

**1. 年龄分布生成**
对于年龄组 i，设其年龄范围为 [a_i, b_i]，目标比例为 p_i，则生成的用户数量为：
n_i = ⌊N × p_i⌋

年龄分布矩阵：
```
Age_Matrix = [
    [4,  12, 0.15],  # 儿童组
    [13, 17, 0.20],  # 青少年组  
    [18, 24, 0.25],  # 青年组
    [25, 34, 0.20],  # 成年早期组
    [35, 44, 0.12],  # 成年中期组
    [45, 54, 0.05],  # 成年后期组
    [55, 75, 0.03]   # 老年组
]
```

**2. 性别分布生成**
男性用户数：n_male = ⌊N × 0.52⌋
女性用户数：n_female = N - n_male

**3. 情感特征分布生成**
对于每个情感特征维度 j（活动水平、从众程度、多样性偏好），设级别 k 的目标比例为 p_jk，则：
n_jk = ⌊N × p_jk⌋

情感特征分布矩阵：
```
Emotion_Matrix = [
    [1, 0.30, 0.25, 0.25],  # 1级比例
    [2, 0.50, 0.50, 0.50],  # 2级比例  
    [3, 0.20, 0.25, 0.25]   # 3级比例
]
# 列分别对应：级别、活动水平、从众程度、多样性偏好
```

**4. 特征关联约束**
年龄与职业的映射函数：
```
f_profession(age) = {
    "student"     if age ≤ 12
    "student"     if 13 ≤ age ≤ 17  
    random(P)     if age ≥ 18
}
```
其中 P 为预定义职业集合，包含50+种职业类型。

**5. 分布验证函数**
对于任意特征维度 j，验证其实际分布是否满足预设比例：
```
validation_score_j = Σ|actual_proportion_ik - target_proportion_ik| / target_proportion_ik
```
其中 validation_score_j < ε（ε为可接受的误差阈值，通常设为0.05）。

**算法流程**：
1. **初始化阶段**：设置随机种子，确保可重现性
2. **分层抽样**：按预设比例生成各年龄组用户数量
3. **特征分配**：为每个用户分配性别和情感特征级别
4. **关联约束**：根据年龄选择合适的职业和爱好
5. **描述生成**：基于所有特征生成个性化描述文本
6. **分布验证**：计算各维度的分布偏差，确保满足预设比例

**数学保证**：
该算法通过分层抽样确保了：
- 年龄分布：Σn_i = N，且 |n_i/N - p_i| < 1/N
- 性别分布：n_male + n_female = N，且 |n_male/N - 0.52| < 1/N  
- 情感特征分布：对于每个维度j，Σn_jk = N，且 |n_jk/N - p_jk| < 1/N

这种数学化的设计确保了生成的用户群体具有统计学上的代表性，为后续的推荐系统实验提供了可靠的用户基础。

**算法特点**：

1. **分层抽样策略**：确保每个特征维度的分布比例严格遵循预设值
2. **相关性约束**：年龄与职业、爱好的选择存在逻辑关联
3. **分布验证**：生成后自动验证是否满足预设比例要求
4. **可重现性**：固定随机种子，确保每次生成结果一致
5. **扩展性**：支持调整用户数量和分布比例参数

**生成流程**：
1. 按比例分配年龄组用户数量
2. 平衡性别分布
3. 分层生成情感特征
4. 根据年龄选择合适的职业爱好
5. 生成个性化描述文本
6. 验证分布比例准确性

这种算法确保了生成的用户群体具有统计学上的代表性，为后续的推荐系统实验提供了可靠的用户基础。

### 2.3 用户画像Prompt示例

用户画像通过详细的描述文本来构建，结合三个情感特征维度的具体表现。例如一个名为Nicholas的26岁男性用户，他是一名荒野向导，热爱攀岩等极限运动。根据情感特征分析：

- **活动水平3级**：作为电影爱好者，Nicholas对电影有着永不满足的渴望，愿意观看几乎所有推荐给他的电影。电影是生活的核心部分，电影推荐对他的存在至关重要。他对推荐系统有很高的容忍度，即使有一些不满意记忆也不容易退出推荐系统。

- **从众程度1级**：作为忠实追随者，Nicholas的评分严重依赖电影历史评分，很少表达独立意见。他通常给出与历史评分相同的评分，倾向于跟随大众的评分趋势。

- **多样性偏好2级**：作为小众探索者，Nicholas偶尔探索不同类型，但主要坚持偏好的电影类型。他会在动作冒险类型的基础上，偶尔尝试其他相关类型，但不会偏离太远。

这种详细的描述帮助LLM更好地理解用户的个性特征、行为模式和偏好倾向，从而生成更准确的推荐和评分预测。

## 3. 物品Prompt构造

### 3.1 物品特征表示

我们为每个物品构建丰富的特征表示，包括：

- **基础信息**：电影标题、上映年份、时长、语言版本、分级信息等基本标识信息
- **内容特征**：主要演员阵容（前5名）、导演信息、电影类型标签（如动作、喜剧、科幻等）、制作国家、原创语言等创作相关信息
- **统计特征**：IMDB平均评分、观看人数、评论数量、票房数据等客观统计指标
- **语义嵌入**：基于电影描述、类型标签和演员信息的预训练语言模型向量表示，支持语义相似性计算和内容推荐

### 3.2 Prompt构造策略

#### 3.2.1 检索历史格式化

系统会构建用户历史交互的格式化描述，按照时间倒序排列（最近观看的排在前面）。每条记录包含：电影标题、上映年份、用户评分（1-9分）、主要类型标签（如动作、冒险、惊悚）、导演姓名、电影简介（前100字符）、主要演员（前3名）等信息。这种结构化的历史信息为LLM提供用户的观影偏好背景和评分模式。

#### 3.2.2 当前物品描述

当前物品的描述包括：电影标题、上映年份、主要类型标签、导演姓名、完整电影简介（200-300字符）、主要演员阵容（前5名）、制作国家、语言版本、时长、分级信息等详细信息。这些信息帮助LLM理解当前需要评分物品的完整特征，为评分推理提供充分的上下文信息。

### 3.3 Few-shot学习示例

我们采用1-shot或2-shot学习策略，提供具体的评分示例来指导LLM的推理过程。系统会根据当前用户的特征和候选电影类型，动态选择最相关的历史评分示例。

**示例1 - 类型偏好冲突**：对于一个12岁对太空探索着迷的用户Alex（活动水平2级，多样性偏好1级），系统会提供他如何评价浪漫音乐剧《La La Land》的示例，说明由于Alex偏好动作冒险和科幻类型，对浪漫音乐剧兴趣较低，因此给出3分的低评分。

**示例2 - 情感一致性**：对于一个25岁喜欢温馨家庭片的用户Sarah（活动水平3级，从众程度2级），系统会提供她如何评价《玩具总动员》的示例，说明由于该片符合她喜欢的温馨、家庭友好类型，且评分理由清晰，因此给出8分的高评分。

这些示例帮助LLM理解如何结合用户画像、历史偏好和当前电影特征进行评分推理，提高评分的一致性和准确性。

## 4. 记忆组件

### 4.1 时间衰减公式

我们实现了基于指数衰减的时间权重机制，用于平衡历史交互的时效性和相关性。记忆评分计算公式为：

**最终评分计算公式**：

$$\text{Final Score} = \text{Content Similarity} \times e^{-\lambda \times \text{Time Distance}} \times (1 + \text{Emotion Consistency Reward})$$

其中：
- **内容相似度**：基于电影类型、导演、演员、描述等特征的余弦相似度，取值范围0-1
- **时间距离**：当前时间与历史交互时间的逻辑距离（以交互次数为单位，而非真实时间）
- **λ（时间衰减系数）**：默认值0.15，控制历史信息的衰减速度，值越大衰减越快
- **情感一致性奖励**：默认值0.2，当用户的情感状态与历史情感标签一致时提供额外奖励

这种机制确保最近的、相关的、情感一致的历史交互对当前推荐有更大的影响权重。

### 4.2 用户情感记忆

#### 4.2.1 情感标签自动推断

系统根据用户评分自动推断情感标签，建立用户的情感状态映射。情感标签采用二维模型：

**效价（Valence）维度**：反映用户对电影的情感倾向
- **正面（Positive）**：评分≥7.0，表示用户对电影持积极态度，感到愉悦、满意
- **中性（Neutral）**：评分4.0-6.9，表示用户对电影持中立态度，无明显情感倾向
- **负面（Negative）**：评分≤4.0，表示用户对电影持消极态度，感到失望、不满

**唤醒度（Arousal）维度**：反映用户的情感强度
- **高唤醒（High）**：评分≥8.0，表示电影强烈激发用户情感，产生强烈的情感反应
- **中等唤醒（Medium）**：评分3.0-7.9，表示电影适度激发用户情感，产生中等强度反应
- **低唤醒（Low）**：评分≤3.0，表示电影很少激发用户情感，产生微弱的情感反应

这种自动推断机制避免了手动标注的主观性，为后续的情感一致性计算提供客观基础。

#### 4.2.2 情感一致性计算

情感一致性计算评估用户当前情感状态与历史情感标签的匹配程度，包括效价一致性和唤醒度一致性两个维度：

**效价一致性计算**：

$$\text{Valence Consistency} = 1 - \frac{\min(2.0, |\text{Current Valence} - \text{Historical Valence}|)}{2.0}$$

**取值范围**：0-1，其中1表示完全一致，0表示完全不一致

**差异映射**：
- 正面 (Positive) = 2
- 中性 (Neutral) = 1  
- 负面 (Negative) = 0
- 最大差异为2

**唤醒度一致性计算**：

$$\text{Arousal Consistency} = 1 - \min(1.0, |\text{Current Arousal} - \text{Historical Arousal}|)$$

**取值范围**：0-1，其中1表示完全一致，0表示完全不一致

**差异映射**：
- 高唤醒 (High) = 2
- 中等唤醒 (Medium) = 1
- 低唤醒 (Low) = 0
- 最大差异为1

**综合情感一致性**：

$$\text{Emotion Consistency Reward} = \text{Base Reward} \times \text{Valence Consistency} + \text{Base Reward} \times 0.5 \times \text{Arousal Consistency}$$

**基础奖励值**：默认0.2，可根据系统需求调整

这种计算机制帮助系统理解用户情感状态的连续性和稳定性，为个性化推荐提供情感维度的支持。

### 4.3 记忆更新机制

每次交互后，系统自动更新用户的情感记忆，维护用户的情感状态时间序列。记忆更新包括以下信息：

**交互记录信息**：
- **基础数据**：用户ID、电影ID、评分（1-9分）、交互时间戳
- **观看统计**：观看次数（累计）、观看时长（如果可用）
- **情感标签**：自动推断的效价标签（正面/中性/负面）、唤醒度标签（高/中/低）

**记忆存储结构**：
- **用户-电影交互表**：记录每个用户对每部电影的详细交互历史
- **情感标签索引**：按效价和唤醒度建立快速检索索引
- **时间序列数据**：维护用户情感状态的时间演化轨迹

**更新触发条件**：
- **新交互产生**：用户对新电影进行评分时
- **评分修改**：用户修改历史评分时
- **批量导入**：从外部数据源导入历史交互数据时

这种自动化的记忆更新机制确保系统始终拥有最新的用户情感状态信息，为个性化推荐提供实时支持。

## 5. LLM Rater

### 5.1 整体Prompt结构

我们采用约束性的CoT-lite（Chain of Thought）prompting策略：

#### 5.1.1 系统提示词

系统提示词定义了LLM的角色和任务要求。LLM需要根据用户画像和观影历史预测用户对电影的评分，并提供1-3个简洁的评分理由。输出格式要求包含理由和评分两个部分。

#### 5.1.2 用户画像描述

用户画像描述包括姓名、年龄、性别、描述文本、职业、爱好、活动水平、从众程度和多样性偏好等详细信息。这些信息帮助LLM理解用户的个性特征。

### 5.2 Chain of Thought推理示例

#### 5.2.1 推理过程

LLM的推理输出包含具体的评分理由。例如，对于Nicholas用户评价《La La Land》，LLM会分析他对肾上腺素刺激动作片的强烈偏好，以及之前对动作片的高评分和对浪漫内容的低评分，最终得出该浪漫音乐剧不符合其寻求刺激个性的结论，给出3分的低评分。

#### 5.2.2 评分校准

系统确保评分在有效范围内。对于0-9尺度，评分被限制在0到9之间；对于1-10尺度，评分被限制在1到10之间；对于1-5尺度，评分被限制在1到5之间。

### 5.3 多尺度评分支持

系统支持多种评分尺度：
- **0-9尺度**：适用于需要包含"无评分"选项的场景
- **1-10尺度**：传统评分系统
- **1-5尺度**：简化评分系统
- **文本评分**：支持"one"到"ten"的文本表示

## 6. 退出系统场景

### 6.1 随机终止机制

设想一位通勤中的读者“Liya”打开应用寻找下一本书：系统先给出一本候选书，她准备阅读前，用户模拟器会以一个很小的概率 p 做一次“是否临时离开”的判定（伯努利抽样，例如接电话或到站下车）；若命中，本次会话（episode）立即结束，不再产生评分与记忆更新；若未命中，她正常给出反馈，推荐器再给出下一本。一次典型过程是：她连续对前两本书给出反馈，第三本在评分前因抽样命中而自然终止。通过在评分前插入这一步随机终止，我们把“随时可能中断”的真实使用情境纳入交互循环，同时保留关键的系统要素（概率 p、伯努利抽样、会话边界、情感记忆与日志记录），既可用于评估“平均会话长度、自然离开率”等指标，也能让训练策略更贴近真实世界的中断分布。

### 6.2 满意度终止机制

在每次推荐后，我们用一个简单直观的规则来判断是否提前结束当次会话（episode）：系统查看用户最近的评分与情感记忆，如果接连出现低分（连续低评分）或最近评分已跌破个性化阈值，同时EMA（指数移动平均）满意度也滑到警戒线以下，便判定用户此刻不满意，结束本轮交互以避免继续打扰；阈值会随个人历史分布与近期表现自适应微调，既不过早终止也不拖延，且同步写入日志（包含触发信号与阈值快照），为后续策略训练与效果评估提供依据。

### 6.3 终止条件参数

我们提供了详细的参数配置，支持灵活的终止策略调整：

**核心参数配置**：

1. **低评分阈值参数**：
   - **默认值**：4.0/9（在0-9评分尺度下）
   - **可配置范围**：2.0 - 6.0
   - **个性化调整**：基于用户历史评分分布自动调整
   - **动态阈值**：根据用户情感状态实时调整

2. **连续低评分控制**：
   - **默认容忍次数**：3次
   - **可配置范围**：1 - 10次
   - **权重衰减**：连续低评分的权重随时间递增
   - **严重程度考虑**：极低评分（1-2分）权重加倍

3. **EMA满意度阈值**：
   - **基础阈值**：0.6（标准化到0-1范围）
   - **动态调整**：基于用户历史满意度自动校准
   - **衰减系数**：α = 0.1（控制历史信息的影响权重）
   - **计算周期**：每5次交互更新一次

**高级控制参数**：

4. **恢复机制**：
   - **高分重置条件**：连续2次评分≥7.0可重置终止状态
   - **部分恢复**：单次高分可降低终止风险等级
   - **恢复延迟**：重置后需要一定步数才能再次触发终止

5. **预热和冷却机制**：
   - **预热步数**：前S步（默认S=5）不触发终止
   - **冷却时间**：终止后需要等待一定步数才能再次评估
   - **渐进式评估**：随着交互次数增加，评估标准逐渐严格

6. **情感状态集成**：
   - **情感一致性奖励**：情感状态与历史一致时降低终止概率
   - **情绪波动容忍**：考虑用户的情感波动模式
   - **个性化容忍度**：基于用户的情感特征调整参数

**参数优化策略**：

- **A/B测试支持**：支持多组参数配置的对比测试
- **在线学习调整**：根据用户反馈动态优化参数
- **性能监控**：实时监控终止机制对推荐效果的影响
- **用户满意度反馈**：收集用户对终止时机的满意度评价

这种多层次的退出机制确保了系统能够准确识别用户的不满状态，在合适的时机终止会话，避免用户继续体验低质量推荐，同时为推荐系统提供了更真实的训练环境。

## 7. 强化学习推荐系统

### 7.1 环境包装器

我们提供了多种环境包装器来适配不同的强化学习算法：

#### 7.1.1 StableBaselineWrapper

StableBaselineWrapper将用户ID转换为one-hot编码，观察空间包含用户ID的one-hot向量和物品交互历史向量。这种编码方式适合需要明确用户身份的场景。

#### 7.1.2 StableBaselineWrapperNum

StableBaselineWrapperNum保持用户ID为数值形式，观察空间包含用户ID的数值表示和物品交互历史向量。这种编码方式更简洁，适合用户数量较少的场景。

### 7.2 支持的强化学习算法

#### 7.2.1 DQN（Deep Q-Network）

DQN使用深度神经网络来近似Q值函数。网络架构采用三层全连接网络，输入维度为特征维度，中间层为64个神经元，输出层为动作空间大小。激活函数使用ReLU，这种架构适合处理离散动作空间的推荐问题。

#### 7.2.2 PPO（Proximal Policy Optimization）

PPO使用Stable-Baselines3库实现，采用多层感知机策略网络。主要超参数包括学习率3e-4、步数2048、批次大小64、训练轮数10、折扣因子0.99等。PPO算法在策略优化方面表现稳定，适合连续动作空间。

#### 7.2.3 A2C（Advantage Actor-Critic）

A2C同样使用Stable-Baselines3库实现，采用多层感知机策略网络。主要超参数包括学习率7e-4、步数5、折扣因子0.99等。A2C算法结合了策略梯度和价值函数方法，适合需要平衡探索和利用的场景。

